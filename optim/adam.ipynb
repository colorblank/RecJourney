{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam优化器\n",
    "\n",
    "## 核心公式\n",
    "\n",
    "1. 一阶矩估计\n",
    "\n",
    "$$\n",
    "m_t = \\beta_{1} m_{t-1} + (1-\\beta_{1}) g_t,\n",
    "$$\n",
    "\n",
    "其中，$\\beta_1$ 是动量的衰减系数，一般设为 0.9； $m_t$ 为1阶矩估计，$g_t$为当前梯度。\n",
    "\n",
    "2. 二阶矩估计\n",
    "\n",
    "$$\n",
    "v_t = \\beta_{2} v_{t-1} + (1-\\beta_{2}) g_t^2\n",
    "$$\n",
    "\n",
    "其中，$\\beta_{2}$是自适应率的衰减系数，一般设为 0.999； $v_t$ 为2阶矩估计。\n",
    "\n",
    "3. 偏差校正\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1-\\beta_{1}^t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{v}_t = \\frac{v_t}{1-\\beta_{2}^t}\n",
    "$$\n",
    "\n",
    "4. 更新参数\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$为学习率，$\\epsilon$为平滑项，一般设为 1e-8。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Adam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        gradients: list[float],\n",
    "        learning_rate: float = 0.001,\n",
    "        beta1: float = 0.9,\n",
    "        beta2: float = 0.999,\n",
    "        epsilon: float = 1e-8,\n",
    "    ) -> None:\n",
    "        self.gradients = gradients\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.m = [0.0] * len(gradients)\n",
    "        self.v = [0.0] * len(gradients)\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        self.t += 1\n",
    "        for i in range(len(self.gradients)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1-self.beta1) * self.gradients[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1-self.beta2) * self.gradients[i] ** 2\n",
    "            m_hat = self.m[i] / ( 1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / ( 1 - self.beta2 ** self.t)\n",
    "            self.gradients[i] -= self.learning_rate * m_hat / (math.sqrt(v_hat) + self.epsilon)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
